# Azure Data Lake Pipeline â€” IngestÃ£o de Dados

Este repositÃ³rio apresenta o pipeline de ingestÃ£o de dados desenvolvido durante o curso **Azure Data Lake Pipeline â€“ IngestÃ£o de Dados** na Alura. O objetivo Ã© demonstrar boas prÃ¡ticas de provisionamento, organizaÃ§Ã£o e movimentaÃ§Ã£o de dados no Azure.

---

## ðŸ“‚ Estrutura de Pastas

```
.
â”œâ”€â”€ data/                        # DiretÃ³rio para dados processados ou extraÃ­dos
â”œâ”€â”€ dados_boston.zip             # Arquivo ZIP contendo dados de Boston
â”œâ”€â”€ save_data_blob_inicial.py    # Script Python de extraÃ§Ã£o, compactaÃ§Ã£o e upload
â”œâ”€â”€ requirements.txt             # DependÃªncias Python

```

---

## ðŸš€ VisÃ£o Geral

1. **Provisionamento de Recursos Azure**  
   - Conta de Storage  
   - Resource Group  
   - Containers de Blob Storage (`raw-data`, `processed-data`)

2. **IngestÃ£o de Dados**  
   - ExtraÃ§Ã£o de dados de fonte externa  
   - CompactaÃ§Ã£o em ZIP  
   - Upload para container `raw-data`

3. **EstruturaÃ§Ã£o do Data Lake**  
   - **Bronze**: dados brutos  
   - **Silver**: dados filtrados e limpos (merge de arquivos)  
   - **Gold**: dados agregados prontos para consumo

4. **OrquestraÃ§Ã£o com Azure Data Factory**  
   - Linked Services entre Blob Storage e Data Lake  
   - Datasets para leitura/escrita nas camadas  
   - Pipeline de cÃ³pia e merge (Bronze â†’ Silver)

---

## ðŸ› ï¸ PrÃ©-requisitos

- Conta no Azure com permissÃ£o de criaÃ§Ã£o de recursos  
- Python 3.8+ e `pip`  
- Git

---

## âš™ï¸ ConfiguraÃ§Ã£o

1. **Clone o repositÃ³rio**  
   ```bash
   git clone https://github.com/seu-usuario/azure-data-lake-pipeline.git
   cd azure-data-lake-pipeline
   ```

2. **Crie e ative um ambiente virtual**  
   ```bash
   python -m venv venv
   source venv/bin/activate    # Linux/Mac
   venv\Scripts\activate       # Windows
   ```

3. **Instale as dependÃªncias**  
   ```bash
   pip install -r requirements.txt
   ```

4. **Defina variÃ¡veis de ambiente** (substitua pelos seus valores)  
   ```bash
   export AZURE_STORAGE_ACCOUNT="NomeDaStorage"
   export AZURE_STORAGE_KEY="SuaChaveDeAcesso"
   export RESOURCE_GROUP="NomeDoResourceGroup"
   export RAW_CONTAINER="raw-data"
   ```



## ðŸ—ï¸ Estrutura do Data Lake

```
data_lake/
â”œâ”€â”€ bronze/   # arquivos ZIP brutos
â”œâ”€â”€ silver/   # dados consolidados e limpos
â””â”€â”€ gold/     # dados agregados para consumo
```

---

## ðŸ”„ Azure Data Factory

1. **Linked Services**  
   - Blob Storage (`raw-data`)  
   - Data Lake (Bronze, Silver, Gold)

2. **Datasets**  
   - `BlobDataset`  â€” leitura de ZIPs em `raw-data`  
   - `BronzeDataset` â€” escrita na camada Bronze  
   - `SilverDataset` â€” escrita na camada Silver

3. **Pipeline**  
   - **Copy Activity**: de `BlobDataset` para `BronzeDataset`  
   - **Merge Activity**: consolidaÃ§Ã£o de arquivos em `SilverDataset`

---

## â–¶ï¸ ExecuÃ§Ã£o

1. Coloque seus arquivos em `./input/` (ou use `dados_boston.zip` na raiz).  
2. Execute:
   ```bash
   python extract_and_upload.py      --input-dir ./input      --zip-name dados_boston.zip      --container "$RAW_CONTAINER"
   ```
3. Verifique o upload no Portal Azure ou via CLI.  
4. No Data Factory, dispare ou agende o pipeline de copy & merge.

---

## ðŸ“ˆ PrÃ³ximos Passos

- Automatizar descompactaÃ§Ã£o na **Bronze**  
- Implementar transformaÃ§Ãµes na **Silver**  
- Desenvolver agregaÃ§Ãµes na **Gold**  
- Configurar triggers de agendamento no Data Factory  
- Adicionar monitoramento e alertas com Azure Monitor



## ðŸ“œ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](./LICENSE).

---

> **Autor:** Gabriel Lopes  
> **Data:** Abril de 2025  
